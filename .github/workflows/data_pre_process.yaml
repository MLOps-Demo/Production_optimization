name: Data-preprocessing-with-Anomaly-Detection

on: [push]

jobs:
  data_pre_processing:
    runs-on: [ubuntu-latest]
    env:
      repo_token: ${{ secrets.GITHUB_TOKEN }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
      AWS_S3_ENDPOINT: https://s3.amazonaws.com
    steps:
      - uses: actions/checkout@v2
      - uses: iterative/setup-cml@v1
      - uses: iterative/setup-dvc@v1
      - uses: actions/setup-python@v4
        with:
          python-version: "3.8"
      - uses: unfor19/install-aws-cli-action@master
        with:
          version: "2.6.1"
      - name: Extract the commit ID
        id: vars
        shell: bash
        run: |
          echo "branch=${GITHUB_REF#refs/heads/}" >> $GITHUB_OUTPUT
          echo "sha_short=$(git rev-parse --short "$GITHUB_SHA")" >> $GITHUB_OUTPUT
          echo "commit_date=$(git show -s --format=%cd --date=format-local:'%Y-%m-%d__%H:%M:%S')" >> $GITHUB_OUTPUT
          git show -s --format=%cd --date=format:'%Y-%m-%d__%H:%M:%S'
      - name: Create Json file with Commit ID
        id: create-json
        uses: jsdaniell/create-json@v1.2.1
        with:
          name: "commit_id.json"
          json: '{"Branch": "${{ steps.vars.outputs.branch }}", "Sha": "${{ steps.vars.outputs.sha_short }}", "Date": "${{ steps.vars.outputs.commit_date }}"}'
      - name: "Copy Anomaly summary csv file from AWS S3"
        run: |
          aws s3 cp commit_id.json s3://dyna-mlops-usecases/production_optimization/deployed_models/production_optimization_bins_oct_2022/Anomaly_detection/commit_id.json
          aws s3 cp s3://dyna-mlops-usecases/production_optimization/deployed_models/production_optimization_bins_oct_2022/Anomaly_detection/commit_based_anomalies_prod_optimization.csv commit_based_anomalies_prod_optimization.csv
        continue-on-error: true
      - name: "Anomaly Detection"
        run: |
          # Intall all dependencies
          pip install pandas
          # Pull latest data from remote.
          dvc pull -r myS3remote

          # Reproduce training and evaluate pipeline
          dvc repro dvc.yaml:create-dataset
          dvc repro dvc.yaml:anomaly-detect

          # Generating the Anomaly summary report csv for the UI
          python3 src/summary_anom_csv.py
          aws s3 cp commit_based_anomalies_prod_optimization.csv s3://dyna-mlops-usecases/production_optimization/deployed_models/production_optimization_bins_oct_2022/Anomaly_detection/commit_based_anomalies_prod_optimization.csv

          # Uploading the anomaly related data to AWS S3 using aws cli command
          aws s3 cp data/anomaly_treated/ s3://dyna-mlops-usecases/production_optimization/deployed_models/production_optimization_bins_oct_2022/Anomaly_detection/ --recursive
          aws s3 cp data/plots_figs/anomaly_detect_plt_pca.png s3://dyna-mlops-usecases/production_optimization/deployed_models/production_optimization_bins_oct_2022/Anomaly_detection/anomaly_detect_plt_pca.png
          aws s3 cp data/plots_figs/anomaly_detect_plt_tsne.png s3://dyna-mlops-usecases/production_optimization/deployed_models/production_optimization_bins_oct_2022/Anomaly_detection/anomaly_detect_plt_tsne.png

          touch status.txt
          #shred --verbose status.txt
          echo "FALSE" > status.txt
          aws s3 cp status.txt s3://dyna-mlops-usecases/production_optimization/deployed_models/production_optimization_bins_oct_2022/Anomaly_detection/status.txt

          # Add figure to report
          cml asset publish data/plots_figs/anomaly_detect_plt_pca.png --md >> report4.md
          cml asset publish data/plots_figs/anomaly_detect_plt_tsne.png --md >> report5.md

          cml comment create report4.md
          cml comment create report5.md
